GMF arguments: Namespace(batch_size=256, dataset='ml-1m', epochs=20, learner='adam', lr=0.001, num_factors=8, num_neg=4, out=1, path='Data/', regs='[0,0]', verbose=1)
Load data done [28.0 s]. #user=6040, #item=3706, #train=994169, #test=6040
Init: HR = 0.0962, NDCG = 0.0434     [12.4 s]
Iteration 0 [86.1 s]: HR = 0.5036, NDCG = 0.2798, loss = 0.3630 [3.7 s]
Iteration 1 [46.2 s]: HR = 0.5639, NDCG = 0.3155, loss = 0.3120 [4.4 s]
Iteration 2 [48.5 s]: HR = 0.5944, NDCG = 0.3360, loss = 0.2898 [5.9 s]
Iteration 3 [48.9 s]: HR = 0.6088, NDCG = 0.3443, loss = 0.2823 [5.2 s]
Iteration 4 [47.2 s]: HR = 0.6182, NDCG = 0.3522, loss = 0.2763 [4.1 s]
Iteration 5 [45.8 s]: HR = 0.6217, NDCG = 0.3572, loss = 0.2736 [3.5 s]
Iteration 6 [41.4 s]: HR = 0.6263, NDCG = 0.3593, loss = 0.2719 [3.5 s]
Iteration 7 [41.7 s]: HR = 0.6262, NDCG = 0.3581, loss = 0.2708 [3.5 s]
Iteration 8 [44.2 s]: HR = 0.6270, NDCG = 0.3598, loss = 0.2703 [3.5 s]
Iteration 9 [44.1 s]: HR = 0.6311, NDCG = 0.3610, loss = 0.2697 [3.7 s]
Iteration 10 [43.7 s]: HR = 0.6328, NDCG = 0.3633, loss = 0.2692 [4.4 s]
Iteration 11 [43.1 s]: HR = 0.6331, NDCG = 0.3630, loss = 0.2689 [3.5 s]
Iteration 12 [45.4 s]: HR = 0.6305, NDCG = 0.3629, loss = 0.2686 [5.9 s]
Iteration 13 [49.1 s]: HR = 0.6329, NDCG = 0.3633, loss = 0.2684 [3.8 s]
Iteration 14 [46.0 s]: HR = 0.6326, NDCG = 0.3637, loss = 0.2683 [3.9 s]
Iteration 15 [55.8 s]: HR = 0.6325, NDCG = 0.3647, loss = 0.2681 [6.9 s]
Iteration 16 [50.8 s]: HR = 0.6361, NDCG = 0.3647, loss = 0.2677 [4.1 s]
Iteration 17 [73.7 s]: HR = 0.6321, NDCG = 0.3633, loss = 0.2676 [6.5 s]
Iteration 18 [61.8 s]: HR = 0.6339, NDCG = 0.3641, loss = 0.2673 [6.5 s]
Iteration 19 [68.7 s]: HR = 0.6358, NDCG = 0.3646, loss = 0.2673 [4.8 s]
End. Best Iteration 16:  HR = 0.6361, NDCG = 0.3647. 
The best GMF model is saved to Pretrain/ml-1m_GMF_8_1588775798.h5
Using Theano backend.
MLP arguments: Namespace(batch_size=256, dataset='ml-1m', epochs=20, layers='[64,32,16,8]', learner='adam', lr=0.001, num_neg=4, out=1, path='Data/', reg_layers='[0,0,0,0]', verbose=1) 
Load data done [30.3 s]. #user=6040, #item=3706, #train=994169, #test=6040
Init: HR = 0.0964, NDCG = 0.0442 [17.1]
Iteration 0 [172.9 s]: HR = 0.5260, NDCG = 0.2926, loss = 0.3441 [8.1 s]
Iteration 1 [276.6 s]: HR = 0.5849, NDCG = 0.3280, loss = 0.3053 [6.9 s]
Iteration 2 [127.2 s]: HR = 0.6096, NDCG = 0.3481, loss = 0.2855 [5.9 s]
Iteration 3 [122.6 s]: HR = 0.6247, NDCG = 0.3604, loss = 0.2744 [6.4 s]
Iteration 4 [112.6 s]: HR = 0.6329, NDCG = 0.3664, loss = 0.2672 [5.6 s]
Iteration 5 [120.5 s]: HR = 0.6488, NDCG = 0.3805, loss = 0.2619 [5.8 s]
Iteration 6 [112.9 s]: HR = 0.6454, NDCG = 0.3800, loss = 0.2579 [5.7 s]
Iteration 7 [110.1 s]: HR = 0.6553, NDCG = 0.3867, loss = 0.2546 [5.6 s]
Iteration 8 [108.7 s]: HR = 0.6571, NDCG = 0.3879, loss = 0.2521 [5.6 s]
Iteration 9 [114.6 s]: HR = 0.6636, NDCG = 0.3946, loss = 0.2498 [6.4 s]
Iteration 10 [173.3 s]: HR = 0.6694, NDCG = 0.3945, loss = 0.2477 [7.4 s]
Iteration 11 [162.3 s]: HR = 0.6631, NDCG = 0.3931, loss = 0.2460 [9.7 s]
Iteration 12 [152.9 s]: HR = 0.6575, NDCG = 0.3908, loss = 0.2447 [8.2 s]
Iteration 13 [192.0 s]: HR = 0.6609, NDCG = 0.3927, loss = 0.2432 [7.0 s]
Iteration 14 [114.5 s]: HR = 0.6752, NDCG = 0.3995, loss = 0.2420 [5.7 s]
Iteration 15 [151.3 s]: HR = 0.6707, NDCG = 0.3962, loss = 0.2409 [5.4 s]
Iteration 16 [149.7 s]: HR = 0.6717, NDCG = 0.3997, loss = 0.2399 [9.7 s]
Iteration 17 [114.8 s]: HR = 0.6724, NDCG = 0.4013, loss = 0.2393 [5.4 s]
Iteration 18 [108.5 s]: HR = 0.6724, NDCG = 0.4010, loss = 0.2383 [5.4 s]
Iteration 19 [110.3 s]: HR = 0.6654, NDCG = 0.3957, loss = 0.2375 [5.9 s]
End. Best Iteration 14:  HR = 0.6752, NDCG = 0.3995. 
The best MLP model is saved to Pretrain/ml-1m_MLP_[64,32,16,8]_1588776966.h5
Using Theano backend.
NeuMF arguments: Namespace(batch_size=256, dataset='ml-1m', epochs=20, layers='[64,32,16,8]', learner='adam', lr=0.001, mf_pretrain='Pretrain/ml-1m_GMF_8_1501651698.h5', mlp_pretrain='Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5', num_factors=8, num_neg=4, out=1, path='Data/', reg_layers='[0,0,0,0]', reg_mf=0, verbose=1) 
Load data done [27.4 s]. #user=6040, #item=3706, #train=994169, #test=6040
Load pretrained GMF (Pretrain/ml-1m_GMF_8_1501651698.h5) and MLP (Pretrain/ml-1m_MLP_[64,32,16,8]_1501652038.h5) models done. 
Init: HR = 0.6710, NDCG = 0.3969
Iteration 0 [168.2 s]: HR = 0.6772, NDCG = 0.3995, loss = 0.2382 [5.9 s]
Iteration 1 [122.0 s]: HR = 0.6745, NDCG = 0.3987, loss = 0.2365 [5.8 s]
Iteration 2 [119.6 s]: HR = 0.6823, NDCG = 0.4034, loss = 0.2354 [5.3 s]
Iteration 3 [125.7 s]: HR = 0.6735, NDCG = 0.4014, loss = 0.2344 [6.5 s]
Iteration 4 [167.2 s]: HR = 0.6823, NDCG = 0.4053, loss = 0.2332 [7.7 s]
Iteration 5 [163.0 s]: HR = 0.6728, NDCG = 0.3976, loss = 0.2321 [6.4 s]
Iteration 6 [204.8 s]: HR = 0.6671, NDCG = 0.3982, loss = 0.2312 [6.8 s]
Iteration 7 [166.4 s]: HR = 0.6773, NDCG = 0.4010, loss = 0.2307 [9.1 s]
Iteration 8 [277.7 s]: HR = 0.6773, NDCG = 0.4005, loss = 0.2296 [11.6 s]
Iteration 9 [178.7 s]: HR = 0.6813, NDCG = 0.4038, loss = 0.2291 [8.7 s]
Iteration 10 [159.2 s]: HR = 0.6772, NDCG = 0.3989, loss = 0.2287 [8.0 s]
Iteration 11 [164.1 s]: HR = 0.6793, NDCG = 0.4040, loss = 0.2276 [7.6 s]
Iteration 12 [165.9 s]: HR = 0.6748, NDCG = 0.4034, loss = 0.2273 [7.8 s]
Iteration 13 [179.4 s]: HR = 0.6730, NDCG = 0.4012, loss = 0.2266 [7.2 s]
Iteration 14 [134.7 s]: HR = 0.6803, NDCG = 0.4048, loss = 0.2258 [6.3 s]
Iteration 15 [133.6 s]: HR = 0.6791, NDCG = 0.4047, loss = 0.2257 [6.1 s]
Iteration 16 [131.3 s]: HR = 0.6815, NDCG = 0.4075, loss = 0.2251 [5.3 s]
Iteration 17 [118.8 s]: HR = 0.6758, NDCG = 0.4011, loss = 0.2248 [5.4 s]
Iteration 18 [119.1 s]: HR = 0.6805, NDCG = 0.4061, loss = 0.2244 [5.8 s]
Iteration 19 [123.1 s]: HR = 0.6724, NDCG = 0.4020, loss = 0.2239 [5.7 s]
End. Best Iteration 2:  HR = 0.6823, NDCG = 0.4034. 
The best NeuMF model is saved to Pretrain/ml-1m_NeuMF_8_[64,32,16,8]_1588779962.h5
